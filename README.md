# **Data Engineering Project**
## **Data Extraction, Storage, and Processing Pipeline**
In this project, I designed and implemented a Pipeline in Python using the **ELT** technique. I used a variety of libraries such as **requests, sqlalchemy, pandas, os, glob, datetime, configparser, json and pprint**, to perform data extractions from a **Rest API** (using the **GET** method), create **dataframes** with the extracted data, store these dataframes in **parquet** format in a local **Datalake** implemented in a file system (and applying some type of partitioning in some cases), and then read them, process them (applying aggregations or some business logic) and store them in a **Datawarehouse** based on **PostgreSQL** hosted in the **cloud** (on an Aiven server).

### Some links:
- [Datawarehouse Structure](https://miro.com/welcomeonboard/bWp2ZmNQQzdNUXZJaE1PMXNHV1JQYWRoajRweU15bVMyaHBydDBJL3Ryc2k2eXlOMkNpZ1h2NXFCcUlhbFlpNGJKNmh3U0F5LzlpQ3BwVVllK1ZJS2xUK2ZwUjVSc1ZDZkd5UER5OGVNZnpTU3RINVk4UkxvditLSTFpaENkVVZzVXVvMm53MW9OWFg5bkJoVXZxdFhRPT0hdjE=?share_link_id=603489510597)
- [How Relational Database Extraction Worksl (SQL)](https://miro.com/welcomeonboard/c0NKU2dyZVB5aFF2VnhjUXk0OEZKV05YbkxnUHdZUHlteTF2U1FJUjhhQk9zQ0E4RytJeVlHQTF0dnRiamowMjc1NEI4N2d0SjVHRlozRG9wQjVWSWxUK2ZwUjVSc1ZDZkd5UER5OGVNZndEWk5FUTFDRjBicTJEQVhPRGVqamVhWWluRVAxeXRuUUgwWDl3Mk1qRGVRPT0hdjE=?share_link_id=42722093757)
